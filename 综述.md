# 基于Transformer的自然语言处理预训练语言模型概述
- 预训练: 预训练指建立基本的模型,先在一些比较基础的数据集,语料库上进行训练,然后按照具体任务训练,学习数据的普遍特征.可以提供很好的初始化.
- 微调: 指在具体的下游任务中使用预训练好的模型进行迁移学习，以获得更好的泛化效果。
- GPT: 生成式预训练模型.基于自回归的语言模型,能够应用在自然语言理解和自然语言生成类任务.
- BERT: 基于深层Transformer的预训练语言模型,充分利用大规模无标注文本挖掘丰富的语义信息.
    
    由多层 Transformer 组成,输入表示由词向量、块向量和位置向量组成.掩码语言模型类似于完形填空任务,直接将输入文本中的词进行掩码处理,通过模型根据上下文信息进行还原,能够避免信息泄露问题.
    
    基于自编码的语言模型.只能完成生成类任务.
- GPT 和 BERT 是基于 Transformer 的 Encoder 和 Decoder层开发的最基础的 TPLM.
- XLNet、RoBERTa 等模型是基于 BERT 的改进模型.
- T5 和贝叶斯加性回归树（Bayesian Additive Regression Trees，BART）等模型是基于 Encoder-Decoder 的生成模型.
- DistilBERT 和 TinyBERT 等模型是通过知识蒸馏方法压缩的预训练模型.
- BART: 是一个用于生成条件文本的模型.使用GLEU激活函数.在文本判别和生成等任务上表现优秀.
- T5(Text-to-text Transfer Transformer): 是谷歌提出的一个适用于文本生成任务的模型,它将不同形式的任务统一为条件生成任务.
- Transformer-XL: 实现对长文本建模的优化.
- XLNet: 解决了BERT中的掩码恢复策略与其下游应用相矛盾的问题.
- Reformer: 减少了模型的占用内存,提升模型对长文本的建模与处理性能.
- Longformer: 利用三种稀疏注意力来降低计算的复杂度.
- 利用 TPLM 预训练再在下游任务微调的目标是既要适应目标任务，又要避免重写预训练模型。
- [快速调用预训练模型](https://github.com/huggingface/transformers)


# Transformer研究现状综述
- 在机器翻译领域, 基于编码器-解码器架构的Seq2Seq模型, 使用RNN结构. 翻译时, 需要RNN对序列“从头看到尾”, 重要词相距较远时, 信息被不断稀释. 
- 《Attention Is All You Need》, 提出了一种新的注意力Seq2Deq模型. 现在的seq2seq模型变成了RNN和attention结合的模型.
- 解决Seq2Seq问题的Transformer模型出现, 用全attention的结构代替了lstm.
