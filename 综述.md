# 基于Transformer的自然语言处理预训练语言模型概述
- 预训练:预训练指建立基本的模型,先在一些比较基础的数据集,语料库上进行训练,然后按照具体任务训练,学习数据的普遍特征.可以提供很好的初始化.
- 微调:指在具体的下游任务中使用预训练好的模型进行迁移学习，以获得更好的泛化效果。
- GPT: 生成式预训练模型.基于自回归的语言模型,能够应用在自然语言理解和自然语言生成类任务.
- BERT: 基于深层Transformer的预训练语言模型,充分利用大规模无标注文本挖掘丰富的语义信息.
    
    由多层 Transformer 组成,输入表示由词向量、块向量和位置向量组成.掩码语言模型类似于完形填空任务,直接将输入文本中的词进行掩码处理,通过模型根据上下文信息进行还原,能够避免信息泄露问题.
    
    基于自编码的语言模型.只能完成生成类任务.
- BART: 是一个用于生成条件文本的模型.使用GLEU激活函数.在文本判别和生成等任务上表现优秀.
- T5(Text-to-text Transfer Transformer): 是谷歌提出的一个适用于文本生成任务的模型,它将不同形式的任务统一为条件生成任务.
- Transformer-XL: 实现对长文本建模的优化.
- XLNet: 解决了BERT中的掩码恢复策略与其下游应用相矛盾的问题.
- Reformer: 减少了模型的占用内存,提升模型对长文本的建模与处理性能.
- Longformer: 利用三种稀疏注意力来降低计算的复杂度.
- 利用 TPLM 预训练再在下游任务微调的目标是既要适应目标任务，又要避免重写预训练模型。
- [快速调用预训练模型](https://github.com/huggingface/transformers)
